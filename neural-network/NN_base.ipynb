{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from typing import Final\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "\n",
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # Suppress TF AVX info message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the complete dataset\n",
    "\n",
    "# Read in the raw data\n",
    "gdp_raw = pd.read_excel('gdp.xlsx').set_axis(\n",
    "    [\n",
    "        'county', '2017', '2018',\n",
    "        '2019', '2020', 'rank 2018',\n",
    "        'percent change 2018', 'percent change 2019',\n",
    "        'percent change 2020', 'rank 2020'\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "gdp_clean: pd.DataFrame = gdp_raw.drop(\n",
    "    columns=[\n",
    "        'rank 2018', '2017', '2018', '2020',\n",
    "        'rank 2020', 'percent change 2018',\n",
    "        'percent change 2019'\n",
    "    ],\n",
    "    inplace=False).iloc[5:3222]\n",
    "\n",
    "state_names = pd.read_csv('us-counties-2020.csv')['state'].unique()\n",
    "counties_df = pd.read_csv('complete.csv')\n",
    "\n",
    "# Initialize empty columns\n",
    "counties_df['2019 raw GDP'] = np.nan  # iloc =5\n",
    "counties_df['percent change 2020'] = np.nan  # iloc = 6\n",
    "\n",
    "# Rearrange the gpd data so that it's ordered by counties\n",
    "curr_state = None\n",
    "for index, row in gdp_clean.iterrows():\n",
    "    if row[0] in state_names:\n",
    "        curr_state = row[0]\n",
    "        continue\n",
    "    else:\n",
    "        row_index = counties_df.index[(counties_df['state'] == curr_state) & (\n",
    "            counties_df['county'] == row[0])].tolist()\n",
    "        counties_df.iloc[[row_index], [5]] = row[1]  # type: ignore\n",
    "        counties_df.iloc[[row_index], [6]] = row[2]  # type: ignore\n",
    "\n",
    "# Datatype conversion\n",
    "counties_df['2019 raw GDP'] = counties_df['2019 raw GDP'].astype('float64')\n",
    "counties_df['percent change 2020'] = counties_df['percent change 2020'].astype(\n",
    "    'float64')\n",
    "\n",
    "print(f\"{len(counties_df['state'].unique())} states\")\n",
    "print(counties_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for the state names\n",
    "encoded_state_names = pd.get_dummies(counties_df['state'])\n",
    "counties_df = counties_df.drop(columns=['state'])\\\n",
    "                        .join(encoded_state_names)\\\n",
    "                        .dropna(axis='index', how='any')\n",
    "print(counties_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some constants\n",
    "DEFAULT_LEARNING_RATE: Final[float] = 0.1\n",
    "DEFAULT_EPOCHS: Final[int] = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildDefaultModel():\n",
    "    SGD_optimizer: Final = tf.keras.optimizers.SGD(\n",
    "        learning_rate=DEFAULT_LEARNING_RATE)\n",
    "    lossFunction = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    model: keras.Sequential = keras.Sequential([\n",
    "        keras.Input(shape=(59)),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=30, activation=tf.nn.relu, name='hidden_layer_1'),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=30, activation=tf.nn.relu, name='hidden_layer_2'),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=15, activation=tf.nn.relu, name='hidden_layer_3'),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=5, activation=tf.nn.relu, name='hidden_layer_4'),\n",
    "        tf.keras.layers.Dense(units=1, activation=tf.nn.relu, name='output'),\n",
    "    ], name='Default_COVID_Regressor')\n",
    "\n",
    "    model.compile(loss=lossFunction, optimizer=SGD_optimizer,\n",
    "                  metrics=['MeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "\n",
    "defaultCovidRegressor = BuildDefaultModel()\n",
    "print(defaultCovidRegressor.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the input dataset X\n",
    "\n",
    "input_attributes = counties_df.drop(\n",
    "    columns=['percent change 2020', 'county'], inplace=False)\n",
    "\n",
    "input_attributes['2019 raw GDP'] = input_attributes['2019 raw GDP'].astype(\n",
    "    'float64')\n",
    "input_attributes['cases'] = input_attributes['cases'].astype('int32')\n",
    "\n",
    "input_attributes['Positivity Rate'] = input_attributes['cases'] / \\\n",
    "    input_attributes['2020 population']\n",
    "input_attributes['Death Rate'] = input_attributes['deaths'] / \\\n",
    "    input_attributes['2020 population']\n",
    "\n",
    "input_attributes.drop(columns=['cases', 'deaths'], inplace=True)\n",
    "\n",
    "# Scale the raw GDP and population individually after the positivity rates\n",
    "# Otherwise the rates also get normalized\n",
    "gdp_scaler = MinMaxScaler()\n",
    "population_scaler = MinMaxScaler()\n",
    "input_attributes['2019 raw GDP'] = gdp_scaler.fit_transform(\n",
    "    np.array(input_attributes['2019 raw GDP']).reshape(-1, 1))\n",
    "input_attributes['2020 population'] = population_scaler.fit_transform(\n",
    "    np.array(input_attributes['2020 population']).reshape(-1, 1))\n",
    "print(input_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize y because NN only outputs from 0 to 1\n",
    "y_scaler = MinMaxScaler()\n",
    "y_vals = y_scaler.fit_transform(\n",
    "    np.array(counties_df['percent change 2020']).reshape(-1, 1)\n",
    ")\n",
    "y_vals = y_vals.reshape(y_vals.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_Train, X_Test, y_Train, y_Test = train_test_split(\n",
    "    input_attributes,\n",
    "    y_vals,\n",
    "    test_size=0.1,\n",
    "    random_state=44\n",
    ")\n",
    "\n",
    "print(X_Train.shape, y_Train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and Train the default model\n",
    "train_default = False\n",
    "if train_default:\n",
    "    startTime = time.time()\n",
    "    defaultCovidRegressor.fit(\n",
    "        x=X_Train,\n",
    "        y=y_Train,\n",
    "        epochs=DEFAULT_EPOCHS,\n",
    "        validation_data=(X_Test, y_Test)\n",
    "    )\n",
    "    endTime = time.time()\n",
    "\n",
    "    print(f'Training took {endTime-startTime} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# raw continuous outputs\n",
    "if train_default:\n",
    "    predicted_y: np.ndarray = defaultCovidRegressor.predict(X_Test)\n",
    "    print(f'MSE of test set is: {mean_squared_error(predicted_y, y_Test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateValidationData(inputDF: pd.DataFrame,\n",
    "                           expectedOutputs: np.ndarray,\n",
    "                           trainIndexes: np.ndarray,\n",
    "                           testIndexes: np.ndarray):\n",
    "    X_Train = inputDF.iloc[trainIndexes]\n",
    "    X_Test = inputDF.iloc[testIndexes]\n",
    "\n",
    "    y_Train = expectedOutputs[trainIndexes]\n",
    "    y_Test = expectedOutputs[testIndexes]\n",
    "\n",
    "    return X_Train, X_Test, y_Train, y_Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do 10-fold validation\n",
    "# This is slow so skip if necessary\n",
    "skip_kfold = True\n",
    "if not skip_kfold:\n",
    "    KFolder = KFold(n_splits=10)\n",
    "    mseScores: list[float] = []\n",
    "    accuracyScores: list[float] = []\n",
    "\n",
    "    for trainIndexes, testIndexes in KFolder.split(input_attributes):\n",
    "        X_Train, X_Test, y_Train, y_Test = generateValidationData(\n",
    "            input_attributes,\n",
    "            y_vals,\n",
    "            trainIndexes,\n",
    "            testIndexes\n",
    "        )\n",
    "\n",
    "        defaultCovidRegressor.fit(\n",
    "            x=X_Train, y=y_Train,\n",
    "            epochs=DEFAULT_EPOCHS,\n",
    "            validation_data=(X_Test, y_Test)\n",
    "        )\n",
    "\n",
    "        predicted_y: np.ndarray = defaultCovidRegressor.predict(X_Test)\n",
    "\n",
    "        mseScores.append(mean_squared_error(y_Test, predicted_y))\n",
    "        print(f'MSE Loss: {mseScores}\\nAverage MSE is: {np.average(mseScores)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSearch Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build generic model that accepts parameters\n",
    "\n",
    "def BuildModel(numNodesLayer1=30,\n",
    "               numNodesLayer2=30,\n",
    "               numNodesLayer3=15,\n",
    "               numNodesLayer4=5,\n",
    "               learningRate=DEFAULT_LEARNING_RATE):\n",
    "    SGD_optimizer: Final = tf.keras.optimizers.SGD(learning_rate=learningRate)\n",
    "    lossFunction: Final = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    model: keras.Sequential = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(59)),\n",
    "            tf.keras.layers.Dense(\n",
    "                units=numNodesLayer1, activation=tf.nn.relu, name='hidden_layer_1'),\n",
    "            tf.keras.layers.Dense(\n",
    "                units=numNodesLayer2, activation=tf.nn.relu, name='hidden_layer_2'),\n",
    "            tf.keras.layers.Dense(\n",
    "                units=numNodesLayer3, activation=tf.nn.relu, name='hidden_layer_3'),\n",
    "            tf.keras.layers.Dense(\n",
    "                units=numNodesLayer4, activation=tf.nn.relu, name='hidden_layer_4'),\n",
    "            tf.keras.layers.Dense(\n",
    "                units=1,\n",
    "                activation=tf.nn.relu,\n",
    "                name='output'\n",
    "            ),\n",
    "        ],\n",
    "        name='COVID_Regressor'\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=lossFunction,\n",
    "        optimizer=SGD_optimizer,\n",
    "        metrics=['mse']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "wrappedCovidRegressor = KerasRegressor(build_fn=BuildModel)\n",
    "\n",
    "# parameters passed to BuildModel(...)\n",
    "param_grid = dict(\n",
    "    nb_epoch=np.array([2, list(range(500, 1000))]),\n",
    "    learningRate=np.array([0.03, 0.05, 0.1, 0.2]),\n",
    "    numNodesLayer1=np.array([13, 23, 30, 35]),\n",
    "    numNodesLayer2=np.array([7, 10, 20, 30]),\n",
    "    numNodesLayer3=np.array([10, 15, 17]),\n",
    "    numNodesLayer4=np.array([5, 6, 7])\n",
    ")\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=wrappedCovidRegressor,\n",
    "                                   param_distributions=param_grid,\n",
    "                                   n_jobs=-1,\n",
    "                                   cv=10)\n",
    "\n",
    "# Extremely slow, do not run this repeatedly\n",
    "random_search_result = random_search.fit(\n",
    "    X_Train,\n",
    "    y_Train,\n",
    "    validation_data=(X_Test, y_Test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results = dict(random_search_result.best_params_)\n",
    "print(tuning_results)\n",
    "\n",
    "best_num_epochs = tuning_results['nb_epoch']\n",
    "del tuning_results['nb_epoch'] # delete this one so tuning_results can be used as kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunedModel = BuildModel(**tuning_results)\n",
    "\n",
    "# Compile and Train the default model\n",
    "startTime = time.time()\n",
    "history = tunedModel.fit(\n",
    "    x=X_Train,\n",
    "    y=y_Train,\n",
    "    epochs=best_num_epochs,\n",
    "    validation_data=(X_Test, y_Test)\n",
    ")\n",
    "endTime = time.time()\n",
    "\n",
    "print(f'Training took {endTime-startTime} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('loss.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = 'packaged_model.pkl'\n",
    "with open(pkl_filename, 'wb') as f:\n",
    "    pickle.dump(tunedModel, f)\n",
    "\n",
    "gdp_scaler_file = 'gdp_scaler.pkl'\n",
    "with open(gdp_scaler_file, 'wb') as f:\n",
    "    pickle.dump(gdp_scaler, f)\n",
    "\n",
    "population_scaler_file = 'population_scaler.pkl'\n",
    "with open(population_scaler_file, 'wb') as f:\n",
    "    pickle.dump(population_scaler, f)\n",
    "\n",
    "y_scaler_file = 'y_scaler.pkl'\n",
    "with open(y_scaler_file, 'wb') as f:\n",
    "    pickle.dump(y_scaler, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
