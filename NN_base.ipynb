{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "\n",
    "from typing import Final\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          county    state  cases  deaths  2020 population 2019 raw GDP  \\\n",
      "0        Autauga  Alabama   4190    48.0          58877.0      1540762   \n",
      "1        Baldwin  Alabama  13601   161.0         233140.0      7134734   \n",
      "2        Barbour  Alabama   1514    32.0          25180.0       729105   \n",
      "3           Bibb  Alabama   1834    46.0          22223.0       380453   \n",
      "4         Blount  Alabama   4641    63.0          59081.0       932215   \n",
      "...          ...      ...    ...     ...              ...          ...   \n",
      "3240  Sweetwater  Wyoming   2966    16.0          42158.0      3677972   \n",
      "3241       Teton  Wyoming   2138     4.0          23347.0      2268742   \n",
      "3242       Uinta  Wyoming   1558     7.0          20441.0       881052   \n",
      "3243    Washakie  Wyoming    780    19.0           7658.0       349686   \n",
      "3244      Weston  Wyoming    476     2.0           6809.0       322576   \n",
      "\n",
      "     percent change 2020  \n",
      "0                   -1.3  \n",
      "1                   -2.1  \n",
      "2                   -5.8  \n",
      "3                    2.1  \n",
      "4                   -5.4  \n",
      "...                  ...  \n",
      "3240               -10.9  \n",
      "3241                -4.1  \n",
      "3242               -10.3  \n",
      "3243                -3.7  \n",
      "3244                -3.1  \n",
      "\n",
      "[3245 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "gdp_raw = pd.read_excel('gdp.xlsx').set_axis(\n",
    "    ['county', '2017', '2018', '2019', '2020', 'rank 2018', 'percent change 2018', 'percent change 2019', 'percent change 2020', 'rank 2020'], axis=1, inplace=False)\n",
    "gdp_clean: pd.DataFrame = gdp_raw.drop(columns=['rank 2018', '2017', '2018', '2020',\n",
    "                                                'rank 2020', 'percent change 2018', 'percent change 2019'], inplace=False).iloc[5:3222]\n",
    "\n",
    "state_names = pd.read_csv('us-counties-2020.csv')['state'].unique()\n",
    "\n",
    "counties_df = pd.read_csv('complete.csv')\n",
    "counties_df['2019 raw GDP'] = pd.NaT  # iloc =5\n",
    "counties_df['percent change 2020'] = pd.NaT  # iloc = 6\n",
    "\n",
    "curr_state = None\n",
    "for index, row in gdp_clean.iterrows():\n",
    "    if row[0] in state_names:\n",
    "        curr_state = row[0]\n",
    "        continue\n",
    "    else:\n",
    "        row_index = counties_df.index[(counties_df['state'] == curr_state) & (\n",
    "            counties_df['county'] == row[0])].tolist()\n",
    "        counties_df.iloc[[row_index], [5]] = row[1]  # type: ignore\n",
    "        counties_df.iloc[[row_index], [6]] = row[2]  # type: ignore\n",
    "\n",
    "counties_df.dropna()\n",
    "print(counties_df)\n",
    "counties_df.to_csv('The FINAL dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some constants\n",
    "DEFAULT_LEARNING_RATE: Final[float] = 0.3\n",
    "DEFAULT_EPOCHS: Final[int] = 500\n",
    "\n",
    "input_attributes = counties_df.drop(columns=['percent change 2020'], inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildDefaultModel():\n",
    "    SGD_optimizer: Final = tf.keras.optimizers.SGD(\n",
    "        learning_rate=DEFAULT_LEARNING_RATE)\n",
    "    lossFunction = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    model: keras.Sequential = keras.Sequential([\n",
    "        keras.Input(shape=(16)),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=12, activation='sigmoid', name='hidden_layer_1'),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=3, activation='sigmoid', name='hidden_layer_2'),\n",
    "        tf.keras.layers.Dense(units=7, activation='sigmoid', name='output'),\n",
    "    ], name='Beans_Classifier')\n",
    "\n",
    "    model.compile(loss=lossFunction, optimizer=SGD_optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "beansClassifier = BuildDefaultModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train, X_Test, y_Train, y_Test = train_test_split(\n",
    "    input_attributes, counties_df['percent change 2020'], test_size=0.1, random_state=44)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and Train\n",
    "startTime = time.time()\n",
    "beansClassifier.fit(\n",
    "    x=X_Train, y=y_Train,\n",
    "    epochs=DEFAULT_EPOCHS,\n",
    "    validation_data=(X_Test, y_Test), verbose='2')\n",
    "endTime = time.time()\n",
    "\n",
    "print(f'Training took {endTime-startTime} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the MSE, Accuracy Score, and Confusion Matrices\n",
    "\n",
    "predicted_y: np.ndarray = beansClassifier.predict(\n",
    "    X_Test)  # raw continuous outputs\n",
    "predicted_y_argmaxed = predicted_y.argmax(axis=1)\n",
    "\n",
    "print(f'MSE of test set is: {mean_squared_error(predicted_y, y_Test)}')\n",
    "print(f'Accuracy: {accuracy_score(predicted_y_argmaxed, y_Test.argmax(1))}')\n",
    "\n",
    "print('Precision & Recall:')\n",
    "print(classification_report(predicted_y_argmaxed, y_Test.argmax(1)))\n",
    "\n",
    "# This prints an array of 7 matrices, each matrix is 2x2 of [[TT, TF], [FT, FF]]\n",
    "# Where each index represents the corresponding class name\n",
    "# in the OneHotEncoding function in the above cell\n",
    "print('Confusion Matrix:')\n",
    "print(multilabel_confusion_matrix(\n",
    "    y_pred=predicted_y_argmaxed, y_true=y_Test.argmax(axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateValidationData(inputDF: pd.DataFrame, expectedOutputs: np.ndarray,\n",
    "                           trainIndexes: np.ndarray,\n",
    "                           testIndexes: np.ndarray):\n",
    "    X_Train = inputDF.iloc[trainIndexes]\n",
    "    X_Test = inputDF.iloc[testIndexes]\n",
    "\n",
    "    y_Train = expectedOutputs[trainIndexes]\n",
    "    y_Test = expectedOutputs[testIndexes]\n",
    "\n",
    "    return X_Train, X_Test, y_Train, y_Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do 10-fold validation\n",
    "\n",
    "KFolder = KFold(n_splits=10)\n",
    "mseScores: list[float] = []\n",
    "accuracyScores: list[float] = []\n",
    "\n",
    "for trainIndexes, testIndexes in KFolder.split(normalizedAttributes):\n",
    "    X_Train, X_Test, y_Train, y_Test = generateValidationData(\n",
    "        normalizedAttributes, encodedClassNames, trainIndexes, testIndexes)\n",
    "\n",
    "    beansClassifier.fit(\n",
    "        x=X_Train, y=y_Train,\n",
    "        epochs=DEFAULT_EPOCHS,\n",
    "        validation_data=(X_Test, y_Test))\n",
    "\n",
    "    predicted_y: np.ndarray = beansClassifier.predict(X_Test)\n",
    "\n",
    "    mseScores.append(mean_squared_error(y_Test, predicted_y))\n",
    "    accuracyScores.append(accuracy_score(\n",
    "        predicted_y.argmax(axis=1), y_Test.argmax(axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Accuracy Scores: {accuracyScores}\\nAverage accuracy is: {np.average(accuracyScores)}\\n')\n",
    "print(f'MSE Loss: {mseScores}\\nAverage MSE is: {np.average(mseScores)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildModel(numNodesLayer1=12, numNodesLayer2=3, learningRate=DEFAULT_LEARNING_RATE):\n",
    "    SGD_optimizer: Final = tf.keras.optimizers.SGD(learning_rate=learningRate)\n",
    "    lossFunction = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    model: keras.Sequential = keras.Sequential([\n",
    "        tf.keras.layers.Dense(\n",
    "            input_dim=6,\n",
    "            units=numNodesLayer1, activation='ReLU', name='hidden_layer_1'),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=numNodesLayer2, activation='ReLU', name='hidden_layer_2'),\n",
    "        tf.keras.layers.Dense(units=1, activation='ReLU', name='output'),\n",
    "    ], name='Beans_Classifier')\n",
    "\n",
    "    model.compile(loss=lossFunction, optimizer=SGD_optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "wrappedBeansClassifier = KerasClassifier(build_fn=BuildModel)\n",
    "\n",
    "# parameters passed to BuildModel(...)\n",
    "param_grid = dict(\n",
    "    nb_epoch=np.array(list(range(500, 1001, 50))),\n",
    "    learningRate=np.array([0.1, 0.3, 0.6]),\n",
    "    numNodesLayer1=np.array([12, 13, 14]),\n",
    "    numNodesLayer2=np.array([3, 4, 5]),\n",
    ")\n",
    "\n",
    "grid = GridSearchCV(estimator=wrappedBeansClassifier,\n",
    "                    param_grid=param_grid, n_jobs=-1, cv=10)\n",
    "\n",
    "grid_result = grid.fit(X_Train, y_Train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big reveal of best parameters\n",
    "grid_result.best_params_"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
